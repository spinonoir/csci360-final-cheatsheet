\section{Decision Trees}
\subsection{Definition:} Decision Trees are a an \emph{interpretable non-parametric supervised learning} method used for classification and regression. Goal: create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.
\subsection{Regression Trees:} Predict a continuous target variable. The tree is built by splitting the source set into subsets based on an attribute value test. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node all has the same value of the target variable, or when splitting no longer adds value to the predictions.
\subsubsection{Recursive Binary Splitting:} The process of splitting the data at a node into two groups that lead to the greatest reduction in RSS where $\mathtt{RSS}= \sum_{j=1}^{J}\sum_{i\in R_j}(y_i-\hat{y}_{R_j})^2$ and $\hat{y}_{R_j}=\frac{1}{N_{R_j}}\sum_{i\in R_j}y_i$. \emph{Gain} is the reduction in RSS from the split.
\subsubsection{Cost Complexity Pruning:} The process of pruning a tree to avoid overfitting. We define a subtree $T \subset T_0$ to be any tree that can be obtained by pruning $T_0$ (removing some of its subtrees). We then define the cost complexity criterion to be \[\mathtt{RSS} =\sum_{m=1}^{|T|}\sum_{i:x_i\in R_m}(y_i-\hat{y}_{R_m})^2+\alpha|T|\] where $|T|$ is the number of terminal nodes of $T$, $R_m$ is the rectangle (region) corresponding to the $m$th terminal node, $\hat{y}_{R_m}$ is the predicted response associated with $R_m$, and $\alpha\geq 0$ is a tuning parameter that controls the size of the tree. We seek the subtree that minimizes this equation.
\subsection{Classification Trees:} Predict a categorical target variable. The tree is built by splitting the source set into subsets based on an attribute value test. This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node all has the same value of the target variable, or when splitting no longer adds value to the predictions.
\subsubsection{Classification Error Rate:} (CER) The proportion of training observations in a region that don't belong to the most common class. \[\mathtt{CER}=1-\max(\hat{p}_{mk})\] where $\hat{p}_{mk}$ is the proportion of training observations in the $m$th region that are from the $k$th class. This may not be a good criterion for tree pruning because it is not sufficiently sensitive for tree growing.
\subsubsection{Gini Index:} The Gini index is a measure of total variance across the $K$ classes. \[\mathtt{Gini}=\sum_{k=1}^{K}\hat{p}_{mk}(1-\hat{p}_{mk})\] The Gini index is referred to as a measure of node purity. A small value indicates that a node contains predominantly observations from a single class. The Gini index is used in the \emph{CART} algorithm.
\subsubsection{Cross-Entropy:} The cross-entropy is a measure of total variance across the $K$ classes. A low value for cross entropy indicates that a node contains predominantly observations from a single class.
 \[\mathtt{Cross-Entropy}=-\sum_{k=1}^{K}\hat{p}_{mk}\log{\hat{p}_{mk}}\] where $\hat{p}_{mk}$ is the proportion of training observations in the $m$th region that are from the $k$th class.
The cross-entropy is used in the \emph{C4.5} algorithm.
\subsubsection{Splitting to Maximize Gain:} The process of splitting the data at a node into two groups that lead to the greatest reduction in impurity. \emph{Gain} is the reduction in impurity from the split. \[\mathtt{Gain}=\mathtt{Impurity}_{\text{parent}}-\sum_{j=1}^{\text{children}}\frac{N_j}{N}\mathtt{Impurity}_{\text{child}_j}\] when we select the Gini index as the measure of impurity this is know as \emph{Gain} and with cross-entropy it is known as \emph{Information Gain}.
\subsection{Determining the Best Split:} We can \emph{determine the best split by considering all possible splits for all features and selecting the split that maximizes the gain}. This is computationally expensive so we can use a greedy approach and consider only a subset of the features at each node. We can also use a random forest to select the best split.
\subsection{Ensemble Methods and Boosting:} Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. Boosting is an ensemble method that fits a sequence of weak learners (models that are only slightly better than random guessing) on repeatedly modified versions of the data. The predictions are then combined through a weighted majority vote (classification) or a weighted sum (regression) to produce the final prediction. A common tree boosting algorithm is \emph{AdaBoost}